{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOBj6TGQ4jTzdBokXQfKeCj",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bayesmaxxing/gpt-text-classifier/blob/main/gpt_classification_finetune.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xyz0hyCPqij8"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\n",
        "from datasets import Dataset\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data\n",
        "gpt_data = pd.read_csv('./training_data_gpts.csv')\n",
        "llama_data = pd.read_csv('./pplx_training_data.csv')\n",
        "\n",
        "training_data = pd.concat(gpt_data, llama_data, ignore_index=True)\n",
        "prompts = pd.read_csv('./prompts.csv')\n",
        "unique_prompts = prompts['prompt'].unique()\n",
        "# Preprocess the data\n",
        "train_prompts, eval_prompts = train_test_split(unique_prompts, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = prompts[prompts['prompt'].isin(train_prompts)]\n",
        "eval_df = prompts[prompts['prompt'].isin(eval)]\n",
        "\n",
        "training_data = Dataset.from_pandas(train_df)\n",
        "eval_data = Dataset.from_pandas(eval_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NW-D_UNVqwJS",
        "outputId": "8cf5bf19-6245-4e04-db2c-cc6c40420216"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[{'generated_text': 'Can you explain lambda functions?\\n\\nA:\\n\\nThe way we use the language of maths is different than the way students use the language of computer science.  Computer programs are written in a different way to that of a human being, and the way we use mathematics is different the way we do this.  The way we use the language of maths is not the way computer scientists write their programs, because computers are the people who write the programs.\\nWhat is important is that we write mathematical programs in the same way as computers do.\\nWhen we write mathematical programs, we look at the mathematics, and we try to apply the mathematics to the problem at hand.  The important part of this is that we use the same terminology that computers use.  We don'}]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up device for GPU usage\n",
        "from torch import cuda\n",
        "device = torch.device(\"cuda\" if cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fjHtR4Iy3l-z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import tokenizer and apply it to the prompts\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n",
        "\n",
        "def preprocess_function(data):\n",
        "  return tokenizer(data[\"text\"], truncation=True) # what's the true column name here?\n",
        "\n",
        "train_data = training_data.map(preprocess_function, batched=True, return_tensors=\"pt\")\n",
        "eval_data = eval_data.map(preprocess_function, batched=True, return_tensors=\"pt\")"
      ],
      "metadata": {
        "id": "hykG_XPlI8mC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Map labels to ids and ids to labels\n",
        "# TODO: check the labels to make sure that they are correct\n",
        "id2label = {0: \"GPT-4o\", 1: \"GPT-3.5-turbo\", 2: \"Llama 3 sonar small\"}\n",
        "label2id = {\"GPT-4o\": 0, \"GPT-3.5\": 1, \"Llama 3 sonar small\": 2}"
      ],
      "metadata": {
        "id": "JINhElUOKDcd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load evaluation metric\n",
        "accuracy = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  predictions, labels = eval_pred\n",
        "  predictions = np.argmax(predictions, axis=1)\n",
        "  return accuracy.compute(predictions=predictions, references=labels)"
      ],
      "metadata": {
        "id": "VMXU9x8TJgqU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\",\n",
        "                                                           num_labels=3, id2label=id2label, label2id=label2id)\n",
        "model.to(device)"
      ],
      "metadata": {
        "id": "6R_IIVQLKeyj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare training arguments and trainer here\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"distilbert_gpt_classifier\",\n",
        "    learning_rate=2e-5,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=16,\n",
        "    num_training_epochs=3,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_data,\n",
        "    eval_dataset=eval_data,\n",
        "    tokenizer=tokenizer,\n",
        "    #data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics\n",
        ")"
      ],
      "metadata": {
        "id": "yfyx80m9THhj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}